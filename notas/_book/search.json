[
  {
    "objectID": "index.html#objetivos-específicos",
    "href": "index.html#objetivos-específicos",
    "title": "Intro a la GenAI en AWS",
    "section": "Objetivos Específicos",
    "text": "Objetivos Específicos\n\nIntroducir los conceptos básicos para la arquitectura, computo, almacenamiento y despliegue de soluciones de inteligencia artificial generativa.\nDesarrollar experiencia práctica con herramientas de aplicacion de inteligencia artificial en AWS.\nHabilitar al alumno para que pueda desarrollar un producto basico de inteligencia artificial y comenzar a desarollar su portafolio de genAI."
  },
  {
    "objectID": "index.html#temario",
    "href": "index.html#temario",
    "title": "Intro a la GenAI en AWS",
    "section": "Temario",
    "text": "Temario\n\nTeoria y conceptos basicos de redes neuronales.\nTeoria y conceptos basicos de inteligencia artificial generativa\nScripting avanzado (codigo limpio)\nCreación y estandarización de ambientes de desarrollo\nIntroduccion a servicios de AWS: Bedrock, Lambda, RDS, S3 y SageMaker\nProyecto 1: Extraccion de datos de archivos con modelo fundacional.\nProyecto 2: Clasificacion de sentimiento usando modelo fundacional.\nProyecto3: Proceso de pregunta y respuesta usando RAG\nLimpieza de ambiente y revision de cuentas"
  },
  {
    "objectID": "index.html#pre-requisitos",
    "href": "index.html#pre-requisitos",
    "title": "Intro a la GenAI en AWS",
    "section": "Pre-requisitos",
    "text": "Pre-requisitos\n**Esta materia no es un curso de programación, parte de la suposición de que el alumno es capaz de programar en python. Las implementaciones que se verán en la clase son avanzadas y se espera que el alumno invierta tiempo estudiando documentación y debuggeando código por su propia cuenta.\n\nProgramación en Python o R.\nMachine learning\nSQL\nGit\nShell"
  },
  {
    "objectID": "index.html#material",
    "href": "index.html#material",
    "title": "Intro a la GenAI en AWS",
    "section": "Material",
    "text": "Material"
  },
  {
    "objectID": "index.html#referencias-principales",
    "href": "index.html#referencias-principales",
    "title": "Intro a la GenAI en AWS",
    "section": "Referencias principales",
    "text": "Referencias principales\nEn particular este curso no sigue una referencia en particular y está inspirado en whitepapers, buenas prácticas, documentación y tutoriales.\n\nAWS Well-Architected Framework\nMachine Learning Lens\nData Science on AWS: Implementing End-to-End, Continuous AI and Machine Learning Pipelines\nDesigning Machine Learning Systems: An Iterative Process for Production-Ready Applications"
  },
  {
    "objectID": "index.html#amazon-web-services-aws",
    "href": "index.html#amazon-web-services-aws",
    "title": "Intro a la GenAI en AWS",
    "section": "Amazon Web Services (AWS)",
    "text": "Amazon Web Services (AWS)\n\nEn este curso trabajaremos en la nube de AWS, para lo cual es necesario que abras una cuenta. En la medida de lo posible trabajermos con el Free Tier."
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "Intro a la GenAI en AWS",
    "section": "Software",
    "text": "Software\nDurante el curso utilizaremos Python. Durante la primera sesion dedicaremos una parte del tiempo a configurar un ambiente de trabajo, es tarea de la primera sesion el resolver el ambiente de trabajo y llegar a la segunda sesion con jupyter lab o visual studio code instalados en tu computadora. La sesion se imparte en un sistema operativo Linux, para los usuarios de windows asegurarse de llegar a la segunda sesion con la instalacion adecuada de Visual Studio Code y con la herramienta de WSL configurada para poder usar los comandos de terminal que se veran en el curso."
  },
  {
    "objectID": "01-Teoria-y-conceptos-basicos-de-redes-neuronales.html#machine-learning",
    "href": "01-Teoria-y-conceptos-basicos-de-redes-neuronales.html#machine-learning",
    "title": "Teoria y conceptos basicos de redes neuronales",
    "section": "Machine Learning",
    "text": "Machine Learning\nRama de la inteligencia artifical en la cual los algoritmos son entrenados usando datos historicos para generar predicciones sobre nuevos datos.\n\nSupervisado (ejemplos?)\nNo supervisado (ejemplos?)\nReinforcement Learning"
  },
  {
    "objectID": "01-Teoria-y-conceptos-basicos-de-redes-neuronales.html#deep-learning-y-redes-neuronales",
    "href": "01-Teoria-y-conceptos-basicos-de-redes-neuronales.html#deep-learning-y-redes-neuronales",
    "title": "Teoria y conceptos basicos de redes neuronales",
    "section": "Deep Learning y Redes Neuronales",
    "text": "Deep Learning y Redes Neuronales\nLas redes neuronales artificiales tienen como concepto imitar el funcionamiento del cerebro al “conectar” neuronas para generar pensamiento.\nConceptos fundamentales:\n\nNeuronas: Cada neurona recibe múltiples entradas (pesoadas), realiza una operación matemática (generalmente suma ponderada) y pasa el resultado por una función de activación. La salida puede ser enviada a muchas otras neuronas.\nCapas de entrada: Reciben los datos iniciales, por ejemplo, palabras convertidas en vectores.\nCapas ocultas: Realizan cálculos internos y extraen características útiles de los datos.\nCapas de salida: Generan la predicción final, como la próxima palabra en un texto.\nPesos: Son los parámetros ajustables que determinan cuánto influencia tiene cada entrada en la activación de una neurona. Inicialmente pueden ser aleatorios y luego ajustados durante el entrenamiento.\nSesgos: Son valores adicionales que se añaden a la suma ponderada para desplazar la función de activación, ayudando a la red a aprender patrones más complejos.\nFunciones de activacion: Funciones que transforman los datos dentro de las capas de las redes neuronales.\n\nReLU (Rectified Linear Unit): f(x) = max(0, x). Es ampliamente utilizada por su simplicidad y eficiencia.\nSigmoid: f(x) = 1 / (1 + e^(-x)). Produce valores entre 0 y 1, útil en tareas binarios.\nTanh: f(x) = (e^x - e^(-x)) / (e^x + e^(-x)). Produce valores entre -1 y 1."
  },
  {
    "objectID": "02-Teoria-y-conceptos-basicos-de-inteligencia-artificial-generativa.html",
    "href": "02-Teoria-y-conceptos-basicos-de-inteligencia-artificial-generativa.html",
    "title": "Teoria y conceptos basicos de inteligencia artificial generativa",
    "section": "",
    "text": "Las redes neuronales requieren un input para generar un output. Ese input debe tener un formato numerico para poder funcionar.\n\nComo logramos representar palabras, imagenes o sonidos como numeros?\n\n\nUsando una transformacion vectorial. La mas frecuente es la generacion de embeddings.\nWord Embedding es una técnica en el Procesamiento del Lenguaje Natural (NLP) que representa palabras como vectores numéricos en un espacio de alta dimensionalidad. En lugar de tratar las palabras como símbolos discretos (como lo haría un diccionario), Word Embedding captura las relaciones semánticas y sintácticas entre las palabras, permitiendo que las computadoras comprendan mejor el significado del lenguaje.\n\nComo funciona?\n\nLa idea principal es que las palabras que se usan en contextos similares tienden a tener significados similares. Word Embedding aprende estas relaciones observando grandes cantidades de texto y ajustando las posiciones de los vectores de las palabras en el espacio vectorial.\n\nEntrenamiento: Se entrena un modelo (por ejemplo, Word2Vec, GloVe, FastText) con un corpus de texto muy grande. El modelo analiza las palabras en su contexto (palabras vecinas) y ajusta los vectores de las palabras para que las palabras que aparecen en contextos similares estén más cerca en el espacio vectorial.\nRepresentación Vectorial: Cada palabra se convierte en un vector numérico de, por ejemplo, 100, 200 o 300 dimensiones. Cada dimensión representa una característica latente aprendida por el modelo.\nRelaciones Semánticas: Los vectores de palabras similares están cerca en el espacio vectorial. Esto permite realizar operaciones matemáticas para encontrar relaciones entre palabras.\n\n\nEjemplo\nImagina que tienes un corpus de texto pequeño con las siguientes frases:\n\nEl gato está durmiendo en la alfombra.\nEl perro está jugando en el jardín.\nLos gatos les gusta la leche.\nLos perros son leales.\n\nDespués de entrenar un modelo de Word Embedding (en realidad, necesitarías muchísimos más datos, pero esto es solo un ejemplo), podríamos obtener los siguientes vectores simplificados para algunas palabras (en un espacio de 2 dimensiones para facilitar la visualización):\n\ngato: [0.8, 0.2]\nperro: [0.7, 0.3]\ndurmiendo: [0.1, 0.9]\njugando: [0.2, 0.8]\n\n¿Qué podemos observar?\nLos vectores de “gato” y “perro” son relativamente cercanos, lo que refleja que son ambos animales domésticos.\nLos vectores de “durmiendo” y “jugando” también son cercanos, ya que ambos son acciones.\n\n\nNota:\nCuales son los elementos basicos necesarios para entrenar un modelo?\n\n\n\n\nArquitectura de transformers\nLos Transformers son una arquitectura de red neuronal que se basa en el mecanismo de “atención” para ponderar la importancia de diferentes partes de la entrada al procesar secuencias de datos, como texto. A diferencia de las redes neuronales recurrentes (RNNs) que procesan la secuencia palabra por palabra, los Transformers procesan toda la secuencia en paralelo, lo que permite un entrenamiento más rápido y un mejor manejo de las dependencias a largo plazo.\nComponentes Clave de la Arquitectura Transformer:\nLa arquitectura Transformer se compone principalmente de dos bloques: el Encoder (Codificador) y el Decoder (Decodificador).\n\nEncoder (Codificador):\n\nFunción:\nEl Encoder se encarga de procesar la secuencia de entrada y generar una representación contextualizada de cada palabra en la secuencia.\nOutput del Encoder: El Encoder produce una secuencia de vectores contextualizados, donde cada vector representa una palabra de la secuencia de entrada teniendo en cuenta su contexto en toda la frase.\n\nDecoder (Decodificador):\n\nFunción: El Decoder se encarga de generar la secuencia de salida, utilizando la representación contextualizada de la entrada proporcionada por el Encoder.\nLinear y Softmax: La salida de la última capa del Decoder se pasa a través de una capa lineal y una función softmax para producir una distribución de probabilidad sobre el vocabulario. La palabra con la mayor probabilidad se selecciona como la siguiente palabra en la secuencia de salida.\n\nMecanismo de Atención (Attention):\n\nClave del Transformer: El mecanismo de atención permite al modelo ponderar la importancia de diferentes partes de la entrada al procesar una secuencia. Funcionamiento: El mecanismo de atención calcula una puntuación de “atención” para cada par de palabras en la secuencia de entrada (o entre la secuencia de entrada y la secuencia de salida en el caso del Decoder). Estas puntuaciones se utilizan para ponderar los vectores de las palabras, dando más peso a las palabras más relevantes. Multi-Head Attention: En lugar de utilizar un solo mecanismo de atención, los Transformers utilizan múltiples “cabezas” de atención en paralelo. Cada cabeza aprende diferentes patrones de relaciones entre las palabras, lo que permite al modelo capturar una gama más amplia de dependencias.\n1) Representacion vectorial del input 2) Representacion vectorial de la importancia relativa de las palabras y el contexto 3) Proceso de entrenamiento para aprender 1 y 2 4) Prediccion de la siguiente palabra mas probable"
  }
]