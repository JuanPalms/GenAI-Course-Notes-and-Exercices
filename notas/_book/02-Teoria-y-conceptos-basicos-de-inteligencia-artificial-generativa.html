<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.340">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Intro a la GenAI en AWS - Teoria y conceptos basicos de inteligencia artificial generativa</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./01-Teoria-y-conceptos-basicos-de-redes-neuronales.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./02-Teoria-y-conceptos-basicos-de-inteligencia-artificial-generativa.html">Teoria y conceptos basicos de inteligencia artificial generativa</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Intro a la GenAI en AWS</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Objetivos Específicos</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-Teoria-y-conceptos-basicos-de-redes-neuronales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Teoria y conceptos basicos de redes neuronales</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-Teoria-y-conceptos-basicos-de-inteligencia-artificial-generativa.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Teoria y conceptos basicos de inteligencia artificial generativa</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#ejemplo" id="toc-ejemplo" class="nav-link active" data-scroll-target="#ejemplo">Ejemplo</a></li>
  <li><a href="#nota" id="toc-nota" class="nav-link" data-scroll-target="#nota">Nota:</a></li>
  <li><a href="#arquitectura-de-transformers" id="toc-arquitectura-de-transformers" class="nav-link" data-scroll-target="#arquitectura-de-transformers">Arquitectura de transformers</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Teoria y conceptos basicos de inteligencia artificial generativa</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Las redes neuronales requieren un input para generar un output. Ese input debe tener un formato numerico para poder funcionar.</p>
<ul>
<li>Como logramos representar palabras, imagenes o sonidos como numeros?</li>
</ul>
<p><img src="imagenes/vectores.png" class="img-fluid" style="width:100.0%"></p>
<p>Usando una transformacion vectorial. La mas frecuente es la generacion de embeddings.</p>
<p>Word Embedding es una técnica en el Procesamiento del Lenguaje Natural (NLP) que representa palabras como vectores numéricos en un espacio de alta dimensionalidad. En lugar de tratar las palabras como símbolos discretos (como lo haría un diccionario), Word Embedding captura las relaciones semánticas y sintácticas entre las palabras, permitiendo que las computadoras comprendan mejor el significado del lenguaje.</p>
<ul>
<li>Como funciona?</li>
</ul>
<p>La idea principal es que las palabras que se usan en contextos similares tienden a tener significados similares. Word Embedding aprende estas relaciones observando grandes cantidades de texto y ajustando las posiciones de los vectores de las palabras en el espacio vectorial.</p>
<ul>
<li><p>Entrenamiento: Se entrena un modelo (por ejemplo, Word2Vec, GloVe, FastText) con un corpus de texto muy grande. El modelo analiza las palabras en su contexto (palabras vecinas) y ajusta los vectores de las palabras para que las palabras que aparecen en contextos similares estén más cerca en el espacio vectorial.</p></li>
<li><p>Representación Vectorial: Cada palabra se convierte en un vector numérico de, por ejemplo, 100, 200 o 300 dimensiones. Cada dimensión representa una característica latente aprendida por el modelo.</p></li>
<li><p>Relaciones Semánticas: Los vectores de palabras similares están cerca en el espacio vectorial. Esto permite realizar operaciones matemáticas para encontrar relaciones entre palabras.</p></li>
</ul>
<section id="ejemplo" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="ejemplo">Ejemplo</h3>
<p>Imagina que tienes un corpus de texto pequeño con las siguientes frases:</p>
<ol type="1">
<li>El gato está durmiendo en la alfombra.</li>
<li>El perro está jugando en el jardín.</li>
<li>Los gatos les gusta la leche.</li>
<li>Los perros son leales.</li>
</ol>
<p>Después de entrenar un modelo de Word Embedding (en realidad, necesitarías muchísimos más datos, pero esto es solo un ejemplo), podríamos obtener los siguientes vectores simplificados para algunas palabras (en un espacio de 2 dimensiones para facilitar la visualización):</p>
<ol type="1">
<li>gato: [0.8, 0.2]</li>
<li>perro: [0.7, 0.3]</li>
<li>durmiendo: [0.1, 0.9]</li>
<li>jugando: [0.2, 0.8]</li>
</ol>
<p>¿Qué podemos observar?</p>
<p>Los vectores de “gato” y “perro” son relativamente cercanos, lo que refleja que son ambos animales domésticos.</p>
<p>Los vectores de “durmiendo” y “jugando” también son cercanos, ya que ambos son acciones.</p>
</section>
<section id="nota" class="level3">
<h3 class="anchored" data-anchor-id="nota">Nota:</h3>
<p>Cuales son los elementos basicos necesarios para entrenar un modelo?</p>
<p><img src="imagenes/gradient.gif" class="img-fluid" style="width:100.0%"></p>
<p><img src="imagenes/gradient_descent.png" class="img-fluid" style="width:100.0%"></p>
</section>
<section id="arquitectura-de-transformers" class="level3">
<h3 class="anchored" data-anchor-id="arquitectura-de-transformers">Arquitectura de transformers</h3>
<p>Los Transformers son una arquitectura de red neuronal que se basa en el mecanismo de “atención” para ponderar la importancia de diferentes partes de la entrada al procesar secuencias de datos, como texto. A diferencia de las redes neuronales recurrentes (RNNs) que procesan la secuencia palabra por palabra, los Transformers procesan toda la secuencia en paralelo, lo que permite un entrenamiento más rápido y un mejor manejo de las dependencias a largo plazo.</p>
<p>Componentes Clave de la Arquitectura Transformer:</p>
<p>La arquitectura Transformer se compone principalmente de dos bloques: el Encoder (Codificador) y el Decoder (Decodificador).</p>
<ol type="1">
<li>Encoder (Codificador):</li>
</ol>
<p>Función:</p>
<p>El Encoder se encarga de procesar la secuencia de entrada y generar una representación contextualizada de cada palabra en la secuencia.</p>
<p>Output del Encoder: El Encoder produce una secuencia de vectores contextualizados, donde cada vector representa una palabra de la secuencia de entrada teniendo en cuenta su contexto en toda la frase.</p>
<ol start="2" type="1">
<li>Decoder (Decodificador):</li>
</ol>
<p>Función: El Decoder se encarga de generar la secuencia de salida, utilizando la representación contextualizada de la entrada proporcionada por el Encoder.</p>
<p>Linear y Softmax: La salida de la última capa del Decoder se pasa a través de una capa lineal y una función softmax para producir una distribución de probabilidad sobre el vocabulario. La palabra con la mayor probabilidad se selecciona como la siguiente palabra en la secuencia de salida.</p>
<ol start="3" type="1">
<li>Mecanismo de Atención (Attention):</li>
</ol>
<p>Clave del Transformer: El mecanismo de atención permite al modelo ponderar la importancia de diferentes partes de la entrada al procesar una secuencia. Funcionamiento: El mecanismo de atención calcula una puntuación de “atención” para cada par de palabras en la secuencia de entrada (o entre la secuencia de entrada y la secuencia de salida en el caso del Decoder). Estas puntuaciones se utilizan para ponderar los vectores de las palabras, dando más peso a las palabras más relevantes. Multi-Head Attention: En lugar de utilizar un solo mecanismo de atención, los Transformers utilizan múltiples “cabezas” de atención en paralelo. Cada cabeza aprende diferentes patrones de relaciones entre las palabras, lo que permite al modelo capturar una gama más amplia de dependencias.</p>
<p><strong>1) Representacion vectorial del input</strong> <strong>2) Representacion vectorial de la importancia relativa de las palabras y el contexto</strong> <strong>3) Proceso de entrenamiento para aprender 1 y 2</strong> <strong>4) Prediccion de la siguiente palabra mas probable</strong></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./01-Teoria-y-conceptos-basicos-de-redes-neuronales.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Teoria y conceptos basicos de redes neuronales</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->



</body></html>